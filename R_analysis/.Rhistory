daphnia_longiremis_train <- daphnia_longiremis_coordinates[fold_longiremis != 1, ] # the other four fifths are training data
#Fit SDM using the Maxent algorithm
daphnia_pulex_model <- maxent(modelEnv, daphnia_pulex_train) #note we just using the training data
daphnia_longiremis_model <- maxent(modelEnv, daphnia_longiremis_train) #note we just using the training data
#Predicted values for every cell in our region
daphnia_pulex_pred <- predict(daphnia_pulex_model, modelEnv)
daphnia_pulex_absence_xy <- select(daphnia_pulex_absence, LONGITUDE:LATITUDE)
head(daphnia_pulex_absence_xy)
e1 <- evaluate(daphnia_pulex_model, p=daphnia_pulex_test, a=daphnia_pulex_absence_xy, x=modelEnv)
plot(e1, 'ROC')
View(daph_present[[2]])
x_lims = c(-138.3500,	-113.7460)
y_lims = c(40.43043 ,		66.17637)
for (i in 1:nspp ) {
model.extent[[i]] = extent(min(daph_present[[i]]$LONGITUDE)-10,max(daph_present[[i]]$LONGITUDE)+10,
min(daph_present[[i]]$LATITUDE)-10,max(daph_present[[i]]$LATITUDE)+10)
modelEnv[[i]] = crop(currentEnv, model.extent[[i]])
modelFutureEnv[[i]] =  crop(futureEnv, model.extent[[i]])
#Get the overall min/max extents. This is to create one common extent
x_lims = c( min( c( x_lims[1], xmin(model.extent[[i]] ) ) ) ,
max(c( x_lims[2], xmax(model.extent[[i]] ) ) )
)
y_lims = c( min( c( y_lims[1], ymin(model.extent[[i]] ) ) ) ,
max(c( y_lims[2], ymax(model.extent[[i]] ) ) )
)
}
x_lims = c(-138.3500,	-113.7460)
y_lims = c(40.43043 ,		66.17637)
for (i in 1:nspp ) {
model.extent[[i]] = extent(min(daph_present[[i]]$LONGITUDE)-10,max(daph_present[[i]]$LONGITUDE)+10,
min(daph_present[[i]]$LATITUDE)-10,max(daph_present[[i]]$LATITUDE)+10)
modelEnv[[i]] = crop(currentEnv, model.extent[[i]])
modelFutureEnv[[i]] =  crop(futureEnv, model.extent[[i]])
#Get the overall min/max extents. This is to create one common extent
x_lims = c( min( c( x_lims[1], xmin(model.extent[[i]] ) ) ) ,
max(c( x_lims[2], xmax(model.extent[[i]] ) ) )
)
y_lims = c( min( c( y_lims[1], ymin(model.extent[[i]] ) ) ) ,
max(c( y_lims[2], ymax(model.extent[[i]] ) ) )
)
}
View(daph_present[[1]])
#==============================================================================
###2.0 Create MaxEnt model
#==============================================================================
model.extent = vector("list",nspp) #Vector of lists to store data
modelEnv = vector("list",nspp) #Vector of lists to store data
modelFutureEnv = vector("list",nspp) #Vector of lists to store data
x_lims = c(-138.3500,	-113.7460)
y_lims = c(40.43043 ,		66.17637)
for (i in 1:nspp ) {
model.extent[[i]] = extent(min(daph_present[[i]]$LONGITUDE)-10,max(daph_present[[i]]$LONGITUDE)+10,
min(daph_present[[i]]$LATITUDE)-10,max(daph_present[[i]]$LATITUDE)+10)
modelEnv[[i]] = crop(currentEnv, model.extent[[i]])
modelFutureEnv[[i]] =  crop(futureEnv, model.extent[[i]])
#Get the overall min/max extents. This is to create one common extent
x_lims = c( min( c( x_lims[1], xmin(model.extent[[i]] ) ) ) ,
max(c( x_lims[2], xmax(model.extent[[i]] ) ) )
)
y_lims = c( min( c( y_lims[1], ymin(model.extent[[i]] ) ) ) ,
max(c( y_lims[2], ymax(model.extent[[i]] ) ) )
)
}
#==============================================================================
###2.1 Model assessment
#==============================================================================
set.seed(1234)
fold = vector("list",nspp) #Vector of lists to store data
daph_coordinates = vector("list",nspp) #Vector of lists to store data
fold = vector("list",nspp) #Vector of lists to store data
daph_test = vector("list",nspp) #Vector of lists to store data
daph_train = vector("list",nspp) #Vector of lists to store data
for (i in 1:nspp ) {
#first, just make a data frame of latitudes and longitudes for the model
daph_coordinates[[i]] = cbind.data.frame(daph_present[[i]]$LONGITUDE,daph_present[[i]]$LATITUDE)
# add an index that makes five random groups of observations
fold[[i]] = kfold(daph_coordinates[[i]], k=5)
# hold out one fifth as test data
daph_test[[i]] = daph_coordinates[[i]][fold[[i]] == 1, ]
# the other four fifths are training data
daph_train[[i]] = daph_coordinates[[i]][fold[[i]] != 1, ]
}
daph_model = vector("list",nspp)
par (mfrow = c(ceiling(nspp/2),2))
for (i in 1:nspp ) {
daph_model[[i]] = maxent(modelEnv[[i]], daph_train[[i]])
plot(daph_model[[i]],
xlab="Variable contribution (%)",
ylab="Bioclimatic variables",
main=" ")
}
options(java.parameters = "-Xmx4g")
library(raster)
library(rgdal)
library(maps)
library(mapdata)
library(dismo)
library(rJava)
library(maptools)
library(jsonlite)
library(rgbif)
d_genus = "daphnia"
d_species = c("pulex*", 'longiremis*','pulicaria*' )
# , 'lumholtzi*','middendorffiana*','magna*', 'rosea*',
nspp = length(d_species)
spp_gbif = vector("list", nspp) #Vector of lists to store gbif data
#==============================================================================
###1.1 Data Access
#==============================================================================
#Current environment from worldclim
currentEnv=raster::getData('worldclim', var="bio", res=2.5, download=T)
#Future environment based on the 8.5 concentration scenario for 2070 from HADGEM2-ES model
futureEnv=raster::getData('CMIP5', var='bio', res=2.5, rcp=85, model='HE', year=70, download=T)
names(futureEnv)=names(currentEnv)
#World map:
data(wrld_simpl)
#Species locations from gbif - for example, the timber rattlesnake
for (i in 1:nspp ) {
spp_gbif[[i]] = gbif(paste(d_genus), paste(d_species[i]) )
}
#==============================================================================
###1.2 Processing GBIF points
#==============================================================================
daph_spp = vector("list",nspp) #Vector of lists to store gbif data
# get rid of occurences without location information
for (i in 1:nspp ) {
daph_spp[[i]] = subset(spp_gbif[[i]], !is.na(lon) & !is.na(lat))
# find and eliminate duplicate locations
dpdups = duplicated(daph_spp[[i]][, c("lon", "lat")])
daph_spp[[i]]  = daph_spp[[i]] [!dpdups, ] #now Daphnia pulex has 764 records
}
#==============================================================================
#Focus in on Canada:
#==============================================================================
daph_sppC = vector("list",nspp) #Vector of lists to store data
for (i in 1:nspp ) {
daph_sppC[[i]] = subset(daph_spp[[i]], country == "Canada")
}
# make initial plot (Canada) for diagnostic purposes
plot(wrld_simpl, xlim=c(min(daph_sppC[[i]]$lon)-1,max(daph_sppC[[i]]$lon)+1),
ylim=c(min(daph_sppC[[i]]$lat)-1,max(daph_sppC[[i]]$lat)+1), axes=TRUE, col="light yellow")
for (i in 1:nspp ) {
points(daph_sppC[[i]]$lon, daph_sppC[[i]]$lat, col=col_use[i], pch=20, cex=0.75)
}
legend("topright",
legend = legend_use, text.font=c(3),
col = col_use,
pch = c(19,19),
bty = "n",
cex = 1.2,
text.col = "black",
inset = c(0.03, 0.01))
View(daph_sppC)
View(daph_sppC[[1]])
View(daph_sppC[[1]])
xlim=c(min(daph_sppC[[i]]$lon)
xlim=c(min(daph_sppC[[i]]$lon))
x_lims = c( min( c( x_lims[1], xmin(model.extent[[i]] ) ) )
x_lims =  min( c( x_lims[1], xmin(model.extent[[i]] ) ) )
for (i in 1:nspp ) {
model.extent[[i]] = extent(min(daph_sppC[[i]]$lon)-10,max(daph_sppC[[i]]$lon)+10,
min(daph_sppC[[i]]$lat)-10,max(daph_sppC[[i]]$lat)+10)
modelEnv[[i]] = crop(currentEnv, model.extent[[i]])
modelFutureEnv[[i]] =  crop(futureEnv, model.extent[[i]])
#Get the overall min/max extents. This is to create one common extent
x_lims = c( min( c( x_lims[1], xmin(model.extent[[i]] ) ) ) ,
max(c( x_lims[2], xmax(model.extent[[i]] ) ) )
)
y_lims = c( min( c( y_lims[1], ymin(model.extent[[i]] ) ) ) ,
max(c( y_lims[2], ymax(model.extent[[i]] ) ) )
)
}
View(model.extent)
View(model.extent)
View(daph_present)
#==============================================================================
###2.0 Create MaxEnt model
#==============================================================================
model.extent = vector("list",nspp) #Vector of lists to store data
modelEnv = vector("list",nspp) #Vector of lists to store data
modelFutureEnv = vector("list",nspp) #Vector of lists to store data
x_lims = c(-140.1667,-47.58333)
y_lims = c(33.75 ,69.125 )
for (i in 1:nspp ) {
model.extent[[i]] = extent(min(daph_sppC[[i]]$lon)-10,max(daph_sppC[[i]]$lon)+10,
min(daph_sppC[[i]]$lat)-10,max(daph_sppC[[i]]$lat)+10)
modelEnv[[i]] = crop(currentEnv, model.extent[[i]])
modelFutureEnv[[i]] =  crop(futureEnv, model.extent[[i]])
#Get the overall min/max extents. This is to create one common extent
x_lims = c( min( c( x_lims[1], xmin(model.extent[[i]] ) ) ) ,
max(c( x_lims[2], xmax(model.extent[[i]] ) ) )
)
y_lims = c( min( c( y_lims[1], ymin(model.extent[[i]] ) ) ) ,
max(c( y_lims[2], ymax(model.extent[[i]] ) ) )
)
}
View(daph_present[[1]])
#==============================================================================
###2.0 Create MaxEnt model
#==============================================================================
model.extent = vector("list",nspp) #Vector of lists to store data
modelEnv = vector("list",nspp) #Vector of lists to store data
modelFutureEnv = vector("list",nspp) #Vector of lists to store data
x_lims = c(	-137.7167,	-113.7460)
y_lims = c(	40.43043,			63.75000)
for (i in 1:nspp ) {
model.extent[[i]] = extent(min(daph_present[[i]]$LONGITUDE)-10,max(daph_present[[i]]$LONGITUDE)+10,
min(daph_present[[i]]$LATITUDE)-10,max(daph_present[[i]]$LATITUDE)+10)
modelEnv[[i]] = crop(currentEnv, model.extent[[i]])
modelFutureEnv[[i]] =  crop(futureEnv, model.extent[[i]])
#Get the overall min/max extents. This is to create one common extent
x_lims = c( min( c( x_lims[1], xmin(model.extent[[i]] ) ) ) ,
max(c( x_lims[2], xmax(model.extent[[i]] ) ) )
)
y_lims = c( min( c( y_lims[1], ymin(model.extent[[i]] ) ) ) ,
max(c( y_lims[2], ymax(model.extent[[i]] ) ) )
)
}
daph_present[[1]][["Daphnia.pulex"]]
options(java.parameters = "-Xmx4g")
library(tidyverse)
library(raster)
library(rgdal)
library(maps)
library(mapdata)
library(dismo)
library(rJava)
library(maptools)
library(jsonlite)
library(rgbif)
library(rlist)
#==============================================================================
# Choose which species to run:
#==============================================================================
d_species = c('Daphnia.pulex', 'Daphnia.longiremis')
#'Daphnia.middendorffiana','Daphnia.ambigua','Daphnia.dentifera', 'Daphnia.schodleri'
nspp = length(d_species)
daph_loewen = vector("list", nspp) #Vector of lists to store Daphnia data
#==============================================================================
#Data Access:
#==============================================================================
#Current environment from worldclim
currentEnv=raster::getData('worldclim', var="bio", res=2.5, download=T)
#Future environment based on the 8.5 concentration scenario for 2070 from HADGEM2-ES model
futureEnv=raster::getData('CMIP5', var='bio', res=2.5, rcp=85, model='HE', year=70, download=T)
names(futureEnv)=names(currentEnv)
#World map:
data(wrld_simpl)
#Daphnia data from Loewen et al. (2018)
for (i in 1:nspp ) {
daph_loewen[[i]]<-read.csv("./Loewen et al. 2018. Ecography_Data_Dryad.csv") %>%
select(d_species[i], 'LATITUDE', 'LONGITUDE', 'RECENT_DATE', 'NAME')
}
#==============================================================================
# Processing points:
#==============================================================================
daph_spp = vector("list",nspp) #Vector of lists to store tidy daphnia data
# get rid of occurences without location information
for (i in 1:nspp ) {
daph_spp[[i]] = subset(daph_loewen[[i]], !is.na(LONGITUDE) & !is.na(LATITUDE) & !is.na(RECENT_DATE))
# find and eliminate duplicate locations
dpdups = duplicated(daph_spp[[i]][, c("LONGITUDE", "LATITUDE")])
daph_spp[[i]]  = daph_spp[[i]] [!dpdups, ]
}
daph_present = vector("list",nspp) #Vector of lists to store daphnia presence records
for (i in 1:nspp ) {
daph_present[[i]] = subset(daph_spp[[i]], daph_spp[[i]][1] == 1)
}
head(daph_present)
daph_absent = vector("list",nspp) #Vector of lists to store daphnia absence records
for (i in 1:nspp ) {
daph_absent[[i]] = subset(daph_spp[[i]], daph_spp[[i]][1] == 0)
daph_absent[[1]]$Daphnia.pulex=NULL
daph_absent[[2]]$Daphnia.longiremis=NULL
daph_absent[[i]]$RECENT_DATE=NULL
daph_absent[[i]]$NAME=NULL
}
#==============================================================================
###1.3 Initial plots
#==============================================================================
# make initial plots (worldwide) for diagnostic purposes
#use presence records
col_use = c("red","blue")
legend_use = c("D.pulex", "D.longiremis")
plot(wrld_simpl, xlim=c(min(daph_spp[[1]]$LONGITUDE)-1,max(daph_spp[[1]]$LONGITUDE)+1),
ylim=c(min(daph_spp[[1]]$LATITUDE)-1,max(daph_spp[[1]]$LATITUDE)+1), axes=TRUE, col="light yellow")
for (i in 1:nspp ) {
points(daph_present[[i]]$LONGITUDE, daph_present[[i]]$LATITUDE, col=col_use[i], pch=20, cex=0.75)
}
legend("topright",
legend = legend_use, text.font=c(3),
col = col_use,
pch = c(19,19),
bty = "n",
cex = 1.2,
text.col = "black",
inset = c(0.01, 0.01))
#==============================================================================
###2.0 Create MaxEnt model
#==============================================================================
model.extent = vector("list",nspp) #Vector of lists to store data
modelEnv = vector("list",nspp) #Vector of lists to store data
modelFutureEnv = vector("list",nspp) #Vector of lists to store data
x_lims = c(	-137.7167,	-113.7460)
y_lims = c(	40.43043,			63.75000)
for (i in 1:nspp ) {
model.extent[[i]] = extent(min(daph_present[[i]]$LONGITUDE)-10,max(daph_present[[i]]$LONGITUDE)+10,
min(daph_present[[i]]$LATITUDE)-10,max(daph_present[[i]]$LATITUDE)+10)
modelEnv[[i]] = crop(currentEnv, model.extent[[i]])
modelFutureEnv[[i]] =  crop(futureEnv, model.extent[[i]])
#Get the overall min/max extents. This is to create one common extent
x_lims = c( min( c( x_lims[1], xmin(model.extent[[i]] ) ) ) ,
max(c( x_lims[2], xmax(model.extent[[i]] ) ) )
)
y_lims = c( min( c( y_lims[1], ymin(model.extent[[i]] ) ) ) ,
max(c( y_lims[2], ymax(model.extent[[i]] ) ) )
)
}
#==============================================================================
###2.1 Model assessment
#==============================================================================
set.seed(1234)
daph_coordinates = vector("list",nspp) #Vector of lists to store data
fold = vector("list",nspp) #Vector of lists to store data
daph_test = vector("list",nspp) #Vector of lists to store data
daph_train = vector("list",nspp) #Vector of lists to store data
for (i in 1:nspp ) {
#first, just make a data frame of latitudes and longitudes for the model
daph_coordinates[[i]] = cbind.data.frame(daph_present[[i]]$LONGITUDE,daph_present[[i]]$LATITUDE)
# add an index that makes five random groups of observations
fold[[i]] = kfold(daph_coordinates[[i]], k=5)
# hold out one fifth as test data
daph_test[[i]] = daph_coordinates[[i]][fold[[i]] == 1, ]
# the other four fifths are training data
daph_train[[i]] = daph_coordinates[[i]][fold[[i]] != 1, ]
}
daph_model = vector("list",nspp)
par (mfrow = c(ceiling(nspp/2),2))
for (i in 1:nspp ) {
daph_model[[i]] = maxent(modelEnv[[i]], daph_train[[i]])
plot(daph_model[[i]],
xlab="Variable contribution (%)",
ylab="Bioclimatic variables",
main=" ")
}
#==============================================================================
###2.4 Predict current suitability
#==============================================================================
daph_pred = vector("list",nspp)
par (mfrow = c(ceiling(nspp/2),2))
for (i in 1:nspp){
#Note: I'm projecting these to the same modelEnv to get a common
#resolution and extent.
daph_pred[[i]]  = predict(daph_model[[i]], modelEnv[[1]])
plot(daph_pred[[i]], main="Predicted Suitability")
plot(wrld_simpl, fill=FALSE, add=TRUE)
points(daph_present[[i]]$LONGITUDE, daph_present[[i]]$LATITUDE, pch="+", cex=0.2)
#Project current suitability to the same extent:
# extent( daphC_pred[[i]]) = c(x_lims,y_lims)
# res( daphC_pred[[i]] ) = res (daphC_pred[[1]])
}
#bg = vector("list",nspp)
ev_bg = vector("list",nspp)
par (mfrow = c(ceiling(nspp),2))
for (i in 1:nspp) {
#bg[[i]] = randomPoints(modelEnv[[i]], 1000)
ev_bg[[i]] = evaluate (daph_model[[i]], p = daph_test[[i]],
a = daph_absent[[i]], x = modelEnv[[i]] )
plot (ev_bg[[i]], 'ROC' )
}
#==============================================================================
###2.0 Create MaxEnt model
#==============================================================================
model.extent = vector("list",nspp) #Vector of lists to store data
modelEnv = vector("list",nspp) #Vector of lists to store data
modelFutureEnv = vector("list",nspp) #Vector of lists to store data
x_lims = c(	-137.7167,	-113.7460)
y_lims = c(	40.43043,			63.75000)
x_lims = c(	-137.7167,	-113.7460)
y_lims = c(	40.43043,			63.75000)
for (i in 1:nspp ) {
model.extent[[i]] = extent(min(daph_spp[[i]]$LONGITUDE)-10,max(daph_spp[[i]]$LONGITUDE)+10,
min(daph_spp[[i]]$LATITUDE)-10,max(daph_spp[[i]]$LATITUDE)+10)
modelEnv[[i]] = crop(currentEnv, model.extent[[i]])
modelFutureEnv[[i]] =  crop(futureEnv, model.extent[[i]])
#Get the overall min/max extents. This is to create one common extent
x_lims = c( min( c( x_lims[1], xmin(model.extent[[i]] ) ) ) ,
max(c( x_lims[2], xmax(model.extent[[i]] ) ) )
)
y_lims = c( min( c( y_lims[1], ymin(model.extent[[i]] ) ) ) ,
max(c( y_lims[2], ymax(model.extent[[i]] ) ) )
)
}
#==============================================================================
###2.1 Model assessment
#==============================================================================
set.seed(1234)
daph_coordinates = vector("list",nspp) #Vector of lists to store data
fold = vector("list",nspp) #Vector of lists to store data
daph_test = vector("list",nspp) #Vector of lists to store data
daph_train = vector("list",nspp) #Vector of lists to store data
for (i in 1:nspp ) {
#first, just make a data frame of latitudes and longitudes for the model
daph_coordinates[[i]] = cbind.data.frame(daph_present[[i]]$LONGITUDE,daph_present[[i]]$LATITUDE)
# add an index that makes five random groups of observations
fold[[i]] = kfold(daph_coordinates[[i]], k=5)
# hold out one fifth as test data
daph_test[[i]] = daph_coordinates[[i]][fold[[i]] == 1, ]
# the other four fifths are training data
daph_train[[i]] = daph_coordinates[[i]][fold[[i]] != 1, ]
}
daph_model = vector("list",nspp)
par (mfrow = c(ceiling(nspp/2),2))
for (i in 1:nspp ) {
daph_model[[i]] = maxent(modelEnv[[i]], daph_train[[i]])
plot(daph_model[[i]],
xlab="Variable contribution (%)",
ylab="Bioclimatic variables",
main=" ")
}
#==============================================================================
###2.4 Predict current suitability
#==============================================================================
daph_pred = vector("list",nspp)
par (mfrow = c(ceiling(nspp/2),2))
for (i in 1:nspp){
#Note: I'm projecting these to the same modelEnv to get a common
#resolution and extent.
daph_pred[[i]]  = predict(daph_model[[i]], modelEnv[[1]])
plot(daph_pred[[i]], main="Predicted Suitability")
plot(wrld_simpl, fill=FALSE, add=TRUE)
points(daph_present[[i]]$LONGITUDE, daph_present[[i]]$LATITUDE, pch="+", cex=0.2)
#Project current suitability to the same extent:
# extent( daphC_pred[[i]]) = c(x_lims,y_lims)
# res( daphC_pred[[i]] ) = res (daphC_pred[[1]])
}
#bg = vector("list",nspp)
ev_bg = vector("list",nspp)
par (mfrow = c(ceiling(nspp),2))
for (i in 1:nspp) {
#bg[[i]] = randomPoints(modelEnv[[i]], 1000)
ev_bg[[i]] = evaluate (daph_model[[i]], p = daph_test[[i]],
a = daph_absent[[i]], x = modelEnv[[i]] )
plot (ev_bg[[i]], 'ROC' )
}
#==============================================================================
###1.3 Initial plots
#==============================================================================
# make initial plots (worldwide) for diagnostic purposes
#use presence records
col_use = c("red","blue")
legend_use = c("D.pulex", "D.longiremis")
plot(wrld_simpl, xlim=c(min(daph_spp[[1]]$LONGITUDE)-1,max(daph_spp[[1]]$LONGITUDE)+1),
ylim=c(min(daph_spp[[1]]$LATITUDE)-1,max(daph_spp[[1]]$LATITUDE)+1), axes=TRUE, col="light yellow")
for (i in 1:nspp ) {
points(daph_present[[i]]$LONGITUDE, daph_present[[i]]$LATITUDE, col=col_use[i], pch=20, cex=0.75)
}
#==============================================================================
###2.0 Create MaxEnt model
#==============================================================================
model.extent = vector("list",nspp) #Vector of lists to store data
modelEnv = vector("list",nspp) #Vector of lists to store data
modelFutureEnv = vector("list",nspp) #Vector of lists to store data
x_lims = c(	-137.7167,	-113.7460)
y_lims = c(	40.43043,			63.75000)
for (i in 1:nspp ) {
model.extent[[i]] = extent(min(daph_spp[[i]]$LONGITUDE)-10,max(daph_spp[[i]]$LONGITUDE)+10,
min(daph_spp[[i]]$LATITUDE)-10,max(daph_spp[[i]]$LATITUDE)+10)
modelEnv[[i]] = crop(currentEnv, model.extent[[i]])
modelFutureEnv[[i]] =  crop(futureEnv, model.extent[[i]])
#Get the overall min/max extents. This is to create one common extent
x_lims = c( min( c( x_lims[1], xmin(model.extent[[i]] ) ) ) ,
max(c( x_lims[2], xmax(model.extent[[i]] ) ) )
)
y_lims = c( min( c( y_lims[1], ymin(model.extent[[i]] ) ) ) ,
max(c( y_lims[2], ymax(model.extent[[i]] ) ) )
)
}
#==============================================================================
###2.1 Model assessment
#==============================================================================
set.seed(1234)
daph_coordinates = vector("list",nspp) #Vector of lists to store data
fold = vector("list",nspp) #Vector of lists to store data
daph_test = vector("list",nspp) #Vector of lists to store data
daph_train = vector("list",nspp) #Vector of lists to store data
for (i in 1:nspp ) {
#first, just make a data frame of latitudes and longitudes for the model
daph_coordinates[[i]] = cbind.data.frame(daph_present[[i]]$LONGITUDE,daph_present[[i]]$LATITUDE)
# add an index that makes five random groups of observations
fold[[i]] = kfold(daph_coordinates[[i]], k=5)
# hold out one fifth as test data
daph_test[[i]] = daph_coordinates[[i]][fold[[i]] == 1, ]
# the other four fifths are training data
daph_train[[i]] = daph_coordinates[[i]][fold[[i]] != 1, ]
}
#==============================================================================
###2.4 Predict current suitability
#==============================================================================
daph_pred = vector("list",nspp)
par (mfrow = c(ceiling(nspp/2),2))
for (i in 1:nspp){
#Note: I'm projecting these to the same modelEnv to get a common
#resolution and extent.
daph_pred[[i]]  = predict(daph_model[[i]], modelEnv[[1]])
plot(daph_pred[[i]], main="Predicted Suitability")
plot(wrld_simpl, fill=FALSE, add=TRUE)
points(daph_present[[i]]$LONGITUDE, daph_present[[i]]$LATITUDE, pch="+", cex=0.2)
#Project current suitability to the same extent:
# extent( daphC_pred[[i]]) = c(x_lims,y_lims)
# res( daphC_pred[[i]] ) = res (daphC_pred[[1]])
}
